{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Global Optimization employing Deep Gaussian Process Regression\n",
    "\n",
    "See DGPR implementations in:\n",
    "\n",
    "https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/KISSGP_Deep_Kernel_Regression_CUDA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import qmc\n",
    "from scipy.special import erf\n",
    "import scipy.io as sio\n",
    "\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, init\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.optim import Adam\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel, RBFKernel\n",
    "from gpytorch.utils.grid import ScaleToBounds\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.settings import use_toeplitz, fast_pred_var\n",
    "from gpytorch.constraints.constraints import GreaterThan\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from bsa import bsa\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ansys.mapdl.core import launch_mapdl\n",
    "def launch_mapdl_on_available_port(starting_port=50052, max_attempts=10):\n",
    "    for i in range(max_attempts):\n",
    "        port = starting_port + i\n",
    "        try:\n",
    "            mapdl = launch_mapdl(port=port)\n",
    "            print(f\"MAPDL launched successfully on port {port}\")\n",
    "            return mapdl\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to launch MAPDL on port {port}: {e}\")\n",
    "    raise RuntimeError(\"Could not launch MAPDL on any available port\")\n",
    "\n",
    "# Use a função para iniciar uma instância do MAPDL\n",
    "mapdl = launch_mapdl_on_available_port()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *GPR Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel())\n",
    "        self.scale_to_bounds = ScaleToBounds(-1., 1.)\n",
    "        \n",
    "        # Store train_y for later use\n",
    "        self.train_outputs = train_y\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_x = self.scale_to_bounds(x)\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_g, val_x, val_g, training_iterations):\n",
    "    # Initialize the models and likelihood\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x=train_x, train_y=train_g, likelihood=likelihood)\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.cuda()\n",
    "    #     likelihood = likelihood.cuda()\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = Adam([\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.005)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # To track loss values\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Training loop with validation\n",
    "    def train():\n",
    "        best_loss, best_val_loss, best_train_loss = 1e4, 1e4, 1e4\n",
    "        patience = int(training_iterations * 0.01)\n",
    "        wait = 0\n",
    "\n",
    "        for epoch in range(training_iterations):\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass and calculate loss on training set\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_g)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            likelihood.eval()\n",
    "            with torch.no_grad():\n",
    "                val_output = model(val_x)\n",
    "                val_loss = -mll(val_output, val_g).item()\n",
    "\n",
    "            # Save the best model based on validation and training loss\n",
    "            training_loss = loss.item()\n",
    "            final_loss = val_loss # val_loss*0.5 + training_loss*0.5\n",
    " \n",
    "            if final_loss < best_loss:\n",
    "                best_loss = final_loss\n",
    "                best_val_loss = val_loss\n",
    "                best_train_loss = training_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'best_model_GPR.pth')\n",
    "                wait = 0  # Reset patience counter when improvement is found\n",
    "            else:\n",
    "                wait += 1  # Increment patience counter if no improvement\n",
    "\n",
    "            # Early stopping\n",
    "            if wait > patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "            # Track losses for plotting\n",
    "            training_losses.append(loss.item())\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            # print(f'Epoch {epoch + 1} - Training Loss: {loss.item()} - Validation Loss: {val_loss}')\n",
    "\n",
    "        print(f'Best Loss: {best_loss} at epoch {best_epoch}. Training loss: {best_train_loss} and val. loss: {best_val_loss}')\n",
    "        return best_val_loss, best_train_loss\n",
    "    best_loss = train()\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(torch.load('best_model_GPR.pth'))\n",
    "\n",
    "    # Set model and likelihood to eval mode for further evaluation\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return model, likelihood, best_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial sampling plan (Latin Hypercube sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LHS_sample(num_points, DIM):\n",
    "    # Number of variables and sampling points\n",
    "    size = int(num_points)\n",
    "\n",
    "    # Generate LHS samples\n",
    "    sampler = qmc.LatinHypercube(d=DIM,\n",
    "                                 optimization=\"random-cd\")\n",
    "    lhs_sample = sampler.random(n=size)\n",
    "\n",
    "    return lhs_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, model):\n",
    "    \"\"\"\n",
    "    Function to calculate the Expected Improvement using Monte Carlo Integration.\n",
    "    \n",
    "    Parameters:\n",
    "        x (array): Individual under evaluation.\n",
    "        y (array): Valor mínimo obtido até o momento.\n",
    "    \n",
    "    Returns:\n",
    "        array: The value of the Expected Improvement.\n",
    "    \"\"\"\n",
    "    # Get the minimum value obtained so far\n",
    "    ymin = torch.min(model.train_outputs)\n",
    "    # ymin = torch.Tensor(ymin)\n",
    "\n",
    "    # Calculate the prediction value and the variance (Ssqr)\n",
    "    with torch.no_grad(), use_toeplitz(False), fast_pred_var():\n",
    "        preds = model(x)\n",
    "    f = preds.mean\n",
    "    s = preds.variance\n",
    "\n",
    "    # Check for any errors that are less than zero (due to numerical error)\n",
    "    s[s < 0] = 0  # Set negative variances to zero\n",
    "\n",
    "    # Calculate the RMSE (Root Mean Square Error)\n",
    "    s = torch.sqrt(s)\n",
    "\n",
    "    # Calculation of Expected Improvement\n",
    "    term1 = (ymin - f) * (0.5 + 0.5 * erf((ymin - f) / (s * torch.sqrt( torch.from_numpy(np.array([2])) ))))\n",
    "    term2 = (1 / torch.sqrt(2 * torch.from_numpy(np.array([np.pi])))) * s * torch.exp(-((ymin - f) ** 2) / (2 * s ** 2))\n",
    "    \n",
    "    return -(term1 + term2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "Your objective function must go inside obj_fun. The input is *xx*, your design variables vector, and the output is the objective function value (your own cost function).\n",
    "\n",
    "You can either import your objective function from a *.py* file using `import obj_fun from my_file` (where *my_file.py* contains your objective function that is called *obj_fun*, already organized to receive a design variable vector (or matrix) as input and output an objective function value) or just replace the *obj_fun* code below by your entire code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Defining variables only once\n",
    "pile_radius = 0.5\n",
    "minimum_distance = pile_radius * 2.5  # 5 times the pile radius\n",
    "width = 9\n",
    "height = 3.5\n",
    "n_piles = 3\n",
    "n_central_piles = round(2)\n",
    "pile_length = 35\n",
    "\n",
    "# AUXILIARY FUNCTIONS\n",
    "def round_to_nearest(value, targets=[0, 0.333, 0.666, 1]): \n",
    "    # Rounds vertical angle to the nearest target value to work within equipment construction limits \n",
    "    # and reduce sample space\n",
    "    return min(targets, key=lambda t: abs(value - t))\n",
    "\n",
    "def distance_between_points(p1, p2): \n",
    "    # Measures the distance between two points to ensure geometric structure limits\n",
    "    return math.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\n",
    "\n",
    "def mirror_points(coordinates, n_central_piles): \n",
    "    # Mirrors points for symmetrical pile placement\n",
    "    mirror = [(x, -y, z) for x, y, z in coordinates[:-int(n_central_piles)]]\n",
    "    return mirror\n",
    "\n",
    "def convert_x(normalized_x): \n",
    "    # Converts normalized values to actual X coordinates\n",
    "    return pile_radius + normalized_x * (width - 2 * pile_radius)\n",
    "\n",
    "def convert_y(normalized_y): \n",
    "    # Converts normalized values to actual Y coordinates\n",
    "    return pile_radius + normalized_y * (height - 2 * pile_radius)\n",
    "\n",
    "def convert_vertical_angles(vertical_angles_array): \n",
    "    # Converts normalized values to vertical angles in degrees\n",
    "    return [np.round(75 + 15 * ang_v, 2) for ang_v in vertical_angles_array]\n",
    "\n",
    "def convert_horizontal_angles(horizontal_angles_array, n_piles): \n",
    "    # Converts normalized values to horizontal angles in degrees\n",
    "    result = [\n",
    "        np.round(225 - 270 * ang_h, 2) if i < n_piles else np.round(180 * ang_h, 2)\n",
    "        for i, ang_h in enumerate(horizontal_angles_array)\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "def round_to_binary(value):\n",
    "    return 1 if value >= 0.5 else 0\n",
    "\n",
    "def spherical_to_cartesian(x0, y0, z0, pile_length, vertical_angle_deg, horizontal_angle_deg): \n",
    "    # Converts spherical coordinates to cartesian coordinates\n",
    "    theta_v = np.deg2rad(vertical_angle_deg)\n",
    "    theta_h = np.deg2rad(horizontal_angle_deg)\n",
    "    \n",
    "    dx = pile_length * np.cos(theta_v) * np.cos(theta_h)\n",
    "    dy = pile_length * np.cos(theta_v) * np.sin(theta_h)\n",
    "    dz = pile_length * np.sin(theta_v)\n",
    "    \n",
    "    x_final = x0 + dx\n",
    "    y_final = y0 + dy\n",
    "    z_final = z0 + dz\n",
    "\n",
    "    return x_final, y_final, z_final\n",
    "\n",
    "def generate_end_points(start_points, vertical_angles, horizontal_angles): \n",
    "    # Generates end points based on spherical directions\n",
    "    final_coordinates = []\n",
    "    for i in range(len(start_points)):\n",
    "        x, y, z = start_points[i]\n",
    "        theta_v, theta_h = vertical_angles[i], horizontal_angles[i]\n",
    "        x_f, y_f, z_f = spherical_to_cartesian(x, y, z, pile_length, theta_v, theta_h)\n",
    "        final_coordinates.append((round(x_f, 2), round(y_f, 2), -1 * round(z_f, 2)))\n",
    "    return final_coordinates\n",
    "\n",
    "def mirror_end_points(points, k):\n",
    "    # Mirrors the end points of piles along Y axis\n",
    "    mirror = []\n",
    "    for (x, y, z) in points[:-k]:\n",
    "        mirror.append((x, -y, z))\n",
    "    return mirror\n",
    "\n",
    "def distance_between_points_r3(p1, p2): \n",
    "    # Ensures geometric/normative limits of the structure\n",
    "    return round(np.linalg.norm(p1 - p2), 2)\n",
    "\n",
    "def find_shortest_distance_between_vectors(start_points, end_points, k, l, length):\n",
    "    # Computes the minimum distance between two pile vectors\n",
    "    vector_k = end_points[k] - start_points[k]\n",
    "    vector_l = end_points[l] - start_points[l]\n",
    "\n",
    "    # Normalize to get direction vectors\n",
    "    vector_k_normalized = np.round(vector_k / length, 2)\n",
    "    vector_l_normalized = np.round(vector_l / length, 2)\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    point_v1_min = None\n",
    "    point_v2_min = None\n",
    "    N1_min = 0\n",
    "    N2_min = 0\n",
    "    distance = 0\n",
    "\n",
    "    for N1 in np.arange(1, length - 1, 1):\n",
    "        for N2 in np.arange(1, length - 1, 1):\n",
    "            point_v1 = start_points[k] + N1 * vector_k_normalized\n",
    "            point_v2 = start_points[l] + N2 * vector_l_normalized\n",
    "\n",
    "            distance = round(distance_between_points_r3(point_v1, point_v2), 2)\n",
    "\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                point_v1_min = point_v1\n",
    "                point_v2_min = point_v2\n",
    "                N1_min = N1\n",
    "                N2_min = N2\n",
    "\n",
    "    return min_distance, point_v1_min, point_v2_min, N1_min, N2_min\n",
    "\n",
    "\n",
    "# Ensures geometric/normative limits of the structure. Attempts to briefly optimize it;\n",
    "# if not feasible, applies a penalty.\n",
    "def check_and_adjust_points(points, pile_length, min_distance, final_points, vertical_angles_converted, horizontal_angles_converted, horizontal_angles, penalty):\n",
    "    max_iterations = 1 * (n_piles + n_central_piles + n_piles)\n",
    "    iteration = 0\n",
    "    final_points = np.array(final_points)\n",
    "    increment = 0.05\n",
    "    trial = 5\n",
    "\n",
    "    for k in np.arange(0, 5, 1):  # Replace with percentage of piles in the future\n",
    "        while iteration <= max_iterations:\n",
    "            point_adjusted = False\n",
    "\n",
    "            for l in np.arange(0, 5, 1):  # Replace with percentage of piles in the future\n",
    "                if k == l:\n",
    "                    continue\n",
    "\n",
    "                point1 = final_points[k]\n",
    "                point2 = final_points[l]\n",
    "                dist = distance_between_points_r3(point1, point2)\n",
    "\n",
    "                min_dist, _, _, _, _ = find_shortest_distance_between_vectors(points, final_points, k, l, pile_length)\n",
    "\n",
    "                if min_dist < 2.1 * pile_radius or dist < 5 * pile_radius:\n",
    "                    point_adjusted = True\n",
    "                    if k <= n_piles:\n",
    "                        horizontal_angles_converted[k] = np.round(horizontal_angles_converted[k] + increment, 2)\n",
    "                        horizontal_angles[k] = np.round(horizontal_angles[k] + increment, 2)\n",
    "                    else:\n",
    "                        # Alternate between 0/180 and 0/1 using (-1)**trial\n",
    "                        if (-1) ** trial == 1:\n",
    "                            horizontal_angles_converted[k] = 180\n",
    "                            horizontal_angles[k] = 1\n",
    "                        else:\n",
    "                            horizontal_angles_converted[k] = 0\n",
    "                            horizontal_angles[k] = 0\n",
    "\n",
    "                    x_new, y_new, z_new = spherical_to_cartesian(\n",
    "                        points[k][0], points[k][1], points[k][2],\n",
    "                        pile_length, vertical_angles_converted[k], horizontal_angles_converted[k]\n",
    "                    )\n",
    "                    x = np.round(x_new, 2)\n",
    "                    y = np.round(y_new, 2)\n",
    "                    z = -1 * np.round(z_new, 2)\n",
    "\n",
    "                    if k < n_piles:\n",
    "                        final_points[k, :] = [x, y, z]\n",
    "                        final_points[k + n_piles + n_central_piles, :] = [x, -y, z]\n",
    "                    else:\n",
    "                        final_points[k, :] = [x, y, z]\n",
    "                    break\n",
    "\n",
    "            if not point_adjusted:\n",
    "                break\n",
    "            trial += 1\n",
    "            \n",
    "            if trial >= max_iterations:\n",
    "                penalty += ((((3 * pile_radius)/(0.7 + min_dist) + (5 * pile_radius)/(0.7 + dist))) / 2)\n",
    "                k += 1\n",
    "                iteration += 1\n",
    "                break\n",
    "\n",
    "    return final_points, horizontal_angles_converted, horizontal_angles, penalty\n",
    "def analysis(initial_points, adjusted_final_points, horizontal_angles_array, vertical_angles_array):  # Structural analysis in ANSYS\n",
    "    mapdl.clear('NOSTART')\n",
    "    mapdl.prep7()\n",
    "\n",
    "    # Title\n",
    "    mapdl.title('Pile and Shell Analysis')\n",
    "\n",
    "    # Define element type (BEAM188) and its properties\n",
    "    mapdl.et(1, 'BEAM188')\n",
    "\n",
    "    # Material properties\n",
    "    elasticity_modulus = 0.9 * 5600 * (40 ** 0.5) * 1e6  # N/m²\n",
    "    mapdl.mp('EX', 1, elasticity_modulus)\n",
    "    mapdl.mp('PRXY', 1, 0.3)  # Poisson's ratio\n",
    "    mapdl.mp('DENS', 1, 2500)  # Density\n",
    "\n",
    "    # Beam section properties\n",
    "    mapdl.sectype(1, 'BEAM', 'CSOLID')\n",
    "    mapdl.secoffset('CENT')\n",
    "    mapdl.secdata(0.5)\n",
    "\n",
    "    # Number of intermediate nodes\n",
    "    num_intermediate_nodes = 9\n",
    "\n",
    "    # Add nodes\n",
    "    node_id = 1\n",
    "    for i in range(2 * n_piles + n_central_piles):\n",
    "        x_start, y_start, z_start = initial_points[i]\n",
    "        x_end, y_end, z_end = adjusted_final_points[i]\n",
    "\n",
    "        mapdl.n(node_id, x_start, y_start, z_start)\n",
    "        node_id_final = node_id + num_intermediate_nodes + 1\n",
    "        mapdl.n(node_id_final, x_end, y_end, z_end)\n",
    "\n",
    "        mapdl.d(node_id_final, 'ALL', 0)\n",
    "        mapdl.fill(node_id, node_id_final, num_intermediate_nodes)\n",
    "        node_id = node_id_final + 1\n",
    "\n",
    "    # Generate elements\n",
    "    element_id = 1\n",
    "    for i in range(2 * n_piles + n_central_piles):\n",
    "        for j in range(1, num_intermediate_nodes + 2):\n",
    "            N1 = j + (num_intermediate_nodes + 2) * i\n",
    "            N2 = N1 + 1\n",
    "            mapdl.en(element_id, N1, N2)\n",
    "            element_id += 1\n",
    "\n",
    "    # Select BEAM188 elements\n",
    "    mapdl.esel('S', 'TYPE', '', 1)\n",
    "\n",
    "    # Count selected elements\n",
    "    num_elem = mapdl.get('num_elem', 'ELEM', 0, 'COUNT')\n",
    "    k = num_elem\n",
    "\n",
    "    # HEADPIECE\n",
    "    mapdl.n(1000, width / 2, 0, 0)\n",
    "    mapdl.n(1001, width / 2, 0, 1.5)\n",
    "    mapdl.en(k + 1, 1000, 1001)\n",
    "\n",
    "    # FENDER\n",
    "    mapdl.n(1002, width, 0, 0)\n",
    "    mapdl.n(1003, width, 0, -1)\n",
    "    mapdl.en(k + 2, 1002, 1003)\n",
    "\n",
    "    # Define SHELL181 element type\n",
    "    mapdl.et(2, 'SHELL181')\n",
    "    mapdl.keyopt(2, 8, 2)  # Elastoplastic\n",
    "    mapdl.keyopt(2, 3, 2)  # Stress accuracy\n",
    "\n",
    "    # Material properties for SHELL181\n",
    "    shell_elasticity_modulus = 0.9 * 5600 * (20 ** 0.5) * 1e6\n",
    "    mapdl.mp('EX', 2, shell_elasticity_modulus)\n",
    "    mapdl.mp('PRXY', 2, 0.2)\n",
    "    mapdl.mp('DENS', 2, 2500)\n",
    "\n",
    "    # Define shell section\n",
    "    mapdl.sectype(2, 'SHELL')\n",
    "    mapdl.secdata(1.5)\n",
    "\n",
    "    # Create rectangle and mesh it\n",
    "    mapdl.rectng(0, width, -3, 3)\n",
    "    mapdl.esize(0.1)\n",
    "    mapdl.amesh('ALL')\n",
    "\n",
    "    # Select SHELL181 elements and assign section\n",
    "    mapdl.esel('S', 'TYPE', '', 2)\n",
    "    mapdl.emodif('ALL', 'SECNUM', 2)\n",
    "\n",
    "    # Merge nodes\n",
    "    mapdl.nsel('S', 'LOC', 'Z', 0, 1e5)\n",
    "    mapdl.nummrg('NODE', 1e-5)\n",
    "    mapdl.nsel('ALL')\n",
    "\n",
    "    # Apply gravity\n",
    "    mapdl.acel(0, 0, -9.81)\n",
    "    mapdl.finish()\n",
    "\n",
    "    # Solution phase\n",
    "    mapdl.slashsolu()\n",
    "    mapdl.antype(0)\n",
    "\n",
    "    # Load application\n",
    "    force = 1_000_000  # 1 MN\n",
    "    load_cases = 12\n",
    "\n",
    "    for i in range(1, load_cases + 2):\n",
    "        rad = ((((i - 1) * (90 / load_cases)) - 90) * math.pi / 180)\n",
    "        FX = math.cos(rad) * force\n",
    "        FY = math.sin(rad) * force\n",
    "\n",
    "        mapdl.allsel('ALL')\n",
    "        mapdl.f(1001, 'FX', FX)\n",
    "        mapdl.f(1001, 'FY', FY)\n",
    "        mapdl.solve()\n",
    "        mapdl.save(f'load_step_{i}')\n",
    "\n",
    "    # Load step 14 (point 1003, FX = -1500 kN)\n",
    "    mapdl.allsel('ALL')\n",
    "    mapdl.f(1003, 'FX', -1_500_000)\n",
    "    mapdl.solve()\n",
    "    mapdl.save('load_step_14')\n",
    "\n",
    "    # Load step 15 (point 1003, FY = 30% of 1500 kN)\n",
    "    mapdl.allsel('ALL')\n",
    "    mapdl.f(1003, 'FY', -1_500_000 * 0.3)\n",
    "    mapdl.solve()\n",
    "    mapdl.save('load_step_15')\n",
    "\n",
    "    mapdl.finish()\n",
    "\n",
    "    # Post-processing\n",
    "    mapdl.post1()\n",
    "    for i in range(1, load_cases + 4):\n",
    "        mapdl.lcdef(i, i, 1)\n",
    "\n",
    "    mapdl.lczero()\n",
    "    mapdl.lcoper(\"ADD\", 14)\n",
    "    mapdl.lcoper(\"ADD\", 15)\n",
    "    mapdl.lcwrite(16)\n",
    "    \n",
    "    n_loadcases = 16\n",
    "    mapdl.lcase(1)\n",
    "\n",
    "    # MAX evaluation\n",
    "    for R in range(2, load_cases + 5):\n",
    "        mapdl.lcoper('MAX', R)\n",
    "        n_loadcases += 1\n",
    "        mapdl.lcwrite(n_loadcases)\n",
    "    mapdl.lcase(n_loadcases)\n",
    "    mapdl.etable('Fx_MAX', 'SMISC', 1, 'MAX')\n",
    "\n",
    "    # MIN evaluation\n",
    "    mapdl.lcase(1)\n",
    "    for R in range(2, load_cases + 5):\n",
    "        mapdl.lcoper('MIN', R)\n",
    "        n_loadcases += 1\n",
    "        mapdl.lcwrite(n_loadcases)\n",
    "    mapdl.lcase(n_loadcases)\n",
    "    mapdl.etable('Fx_MIN', 'SMISC', 1, 'MIN')\n",
    "\n",
    "    # ABSMAX evaluation\n",
    "    mapdl.lcase(1)\n",
    "    for R in range(2, 17):\n",
    "        n_loadcases += 1\n",
    "        mapdl.lcoper('ABMX', R)\n",
    "        mapdl.lcwrite(n_loadcases)\n",
    "    mapdl.lcase(n_loadcases)\n",
    "    mapdl.etable('My_ABMX', 'SMISC', 2, 'ABMX')\n",
    "    mapdl.etable('Mz_ABMX', 'SMISC', 3, 'ABMX')\n",
    "    mapdl.etable('UX', 'U', 'X')\n",
    "    mapdl.etable('UY', 'U', 'Y')\n",
    "    mapdl.etable('UZ', 'U', 'Z')\n",
    "\n",
    "    # Element sequence\n",
    "    element_sequence = []\n",
    "    j = 1\n",
    "    for i in range(0, (2 * n_piles) + n_central_piles):\n",
    "        y = num_intermediate_nodes * i + j\n",
    "        element_sequence.append(y)\n",
    "        j += 1\n",
    "\n",
    "    # Result storage\n",
    "    data = {\n",
    "        \"Fx_MAX\": [],\n",
    "        \"Fx_MIN\": [],\n",
    "        \"Utot\": [],\n",
    "        \"My\": [],\n",
    "        \"Mz\": [],\n",
    "        \"r_max\": [],\n",
    "        \"load_case_max\": []\n",
    "    }\n",
    "\n",
    "    for elem in element_sequence:\n",
    "        fx_max = mapdl.get_value('ELEM', elem, 'ETABLE', 'Fx_MAX') / 1000\n",
    "        fx_min = mapdl.get_value('ELEM', elem, 'ETABLE', 'Fx_MIN') / 1000\n",
    "        ux = mapdl.get_value('ELEM', elem, 'ETABLE', 'UX') * 1000\n",
    "        uy = mapdl.get_value('ELEM', elem, 'ETABLE', 'UY') * 1000\n",
    "        uz = mapdl.get_value('ELEM', elem, 'ETABLE', 'UZ') * 1000\n",
    "        Utot = math.sqrt(ux**2 + uy**2 + uz**2)\n",
    "\n",
    "        data[\"Fx_MAX\"].append(fx_max)\n",
    "        data[\"Fx_MIN\"].append(fx_min)\n",
    "        data[\"Utot\"].append(Utot)\n",
    "\n",
    "    for elem in element_sequence:\n",
    "        r_max_elem = 0\n",
    "        my_max_elem = 0\n",
    "        mz_max_elem = 0\n",
    "        load_case_max = 0\n",
    "\n",
    "        for load_case in range(1, load_cases + 2):\n",
    "            mapdl.lcase(load_case)\n",
    "            mapdl.etable('My', 'SMISC', 2)\n",
    "            mapdl.etable('Mz', 'SMISC', 3)\n",
    "\n",
    "            my_val = mapdl.get_value('ELEM', elem, 'ETABLE', 'My') / 1000\n",
    "            mz_val = mapdl.get_value('ELEM', elem, 'ETABLE', 'Mz') / 1000\n",
    "            r_val = math.sqrt(my_val**2 + mz_val**2)\n",
    "\n",
    "            if r_val > r_max_elem:\n",
    "                r_max_elem = r_val\n",
    "                my_max_elem = my_val\n",
    "                mz_max_elem = mz_val\n",
    "                load_case_max = load_case\n",
    "\n",
    "        data[\"My\"].append(my_max_elem)\n",
    "        data[\"Mz\"].append(mz_max_elem)\n",
    "        data[\"r_max\"].append(r_max_elem)\n",
    "        data[\"load_case_max\"].append(load_case_max)\n",
    "\n",
    "    df = pd.DataFrame(data).round(1)\n",
    "\n",
    "    def wp(x):\n",
    "        if x <= 0:\n",
    "            return -x**2\n",
    "        elif x > 0:\n",
    "            return x**2\n",
    "        return np.nan\n",
    "\n",
    "    delta1, delta2, delta3, delta4 = 5500, 2000, 2500, 125\n",
    "\n",
    "    def calculate_Pt(df):\n",
    "        df['Pt_Fx_MAX'] = 0.0\n",
    "        df['Pt_Fx_MIN'] = 0.0\n",
    "        df['Pt_r_max'] = 0.0\n",
    "        df['Pt_Utot'] = 0.0\n",
    "\n",
    "        p_total = 0\n",
    "        p_array = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            df.at[index, 'Pt_Fx_MAX'] = wp((abs(row['Fx_MAX']) - delta1) / delta1 if row['Fx_MAX'] > 0 else (abs(row['Fx_MAX']) - delta2) / delta2)\n",
    "            df.at[index, 'Pt_Fx_MIN'] = wp((abs(row['Fx_MIN']) - delta1) / delta1 if row['Fx_MIN'] > 0 else (abs(row['Fx_MIN']) - delta2) / delta2)\n",
    "            df.at[index, 'Pt_r_max'] = wp((abs(row['r_max']) - delta3) / delta3)\n",
    "            df.at[index, 'Pt_Utot'] = wp((abs(row['Utot']) - delta4) / delta4)\n",
    "\n",
    "            p_current = max(df.at[index, 'Pt_Fx_MAX'], df.at[index, 'Pt_Fx_MIN'], df.at[index, 'Pt_r_max'], df.at[index, 'Pt_Utot'])\n",
    "            p_array.append(p_current)\n",
    "            p_total = max(p_array)\n",
    "\n",
    "        return p_total\n",
    "\n",
    "    p_total = calculate_Pt(df)\n",
    "\n",
    "    return p_total, initial_points, adjusted_final_points, horizontal_angles_array, vertical_angles_array\n",
    "# Updated version of obj_fun with float conversion to avoid tensor-related errors\n",
    "# Now using `.item()` to safely extract values from PyTorch tensors\n",
    "# Supports both single and batch evaluations\n",
    "\n",
    "def obj_fun(xx):\n",
    "    p = 0\n",
    "    results = []  # Accumulate evaluation values\n",
    "\n",
    "    # If input is 0-D tensor, convert to 1-D\n",
    "    if xx.dim() == 0:\n",
    "        xx = xx.unsqueeze(0)\n",
    "\n",
    "    # Iterate through each row (for 2D tensors) or handle single input\n",
    "    for i in range(xx.shape[0]):\n",
    "        p = 0\n",
    "        line = xx[i] if xx.dim() > 1 else xx\n",
    "\n",
    "        # Extract normalized input variables\n",
    "        x_aba_1, x_aba_2, x_aba_3, x_central_1, x_central_2 = (\n",
    "            round(float(line[0].item()), 1), round(float(line[1].item()), 1),\n",
    "            round(float(line[2].item()), 1), round(float(line[3].item()), 1),\n",
    "            round(float(line[4].item()), 1)\n",
    "        )\n",
    "\n",
    "        y_aba_1, y_aba_2, y_aba_3 = (\n",
    "            round(float(line[5].item()), 1), round(float(line[6].item()), 1),\n",
    "            round(float(line[7].item()), 1)\n",
    "        )\n",
    "\n",
    "        # Define initial coordinates\n",
    "        coordinates = [\n",
    "            [convert_x(x_aba_1), convert_y(y_aba_1), 0], \n",
    "            [convert_x(x_aba_2), convert_y(y_aba_2), 0],\n",
    "            [convert_x(x_aba_3), convert_y(y_aba_3), 0],\n",
    "            [convert_x(x_central_1), 0.0, 0.0], \n",
    "            [convert_x(x_central_2), 0.0, 0.0]\n",
    "        ]\n",
    "\n",
    "        # Process vertical and horizontal angles\n",
    "        ang_v1, ang_v2, ang_v3, ang_v4, ang_v5 = (\n",
    "            round_to_nearest(float(line[9].item())),\n",
    "            round_to_nearest(float(line[10].item())),\n",
    "            round_to_nearest(float(line[11].item())),\n",
    "            round_to_nearest(float(line[12].item())),\n",
    "            round_to_nearest(float(line[13].item()))\n",
    "        )\n",
    "\n",
    "        ang_h1, ang_h2, ang_h3, ang_h4, ang_h5 = (\n",
    "            float(line[14].item()),\n",
    "            float(line[15].item()),\n",
    "            float(line[16].item()),\n",
    "            round_to_binary(float(line[17].item())),\n",
    "            round_to_binary(float(line[18].item()))\n",
    "        )\n",
    "\n",
    "        # Store angles\n",
    "        vertical_angles = [ang_v1, ang_v2, ang_v3, ang_v4, ang_v5]\n",
    "        horizontal_angles = [ang_h1, ang_h2, ang_h3, ang_h4, ang_h5]\n",
    "\n",
    "        # Convert angles to degrees\n",
    "        vertical_angles_deg = convert_vertical_angles(vertical_angles)\n",
    "        horizontal_angles_deg = convert_horizontal_angles(horizontal_angles, n_piles)\n",
    "\n",
    "        # Mirror base coordinates\n",
    "        mirror = mirror_points(coordinates, n_central_piles)\n",
    "\n",
    "        # Penalize for points that are too close\n",
    "        for i in range(len(coordinates)):\n",
    "            for j in range(i + 1, len(coordinates)):\n",
    "                dist = distance_between_points(coordinates[i], coordinates[j])\n",
    "                if dist < minimum_distance:\n",
    "                    p += (minimum_distance / (0.5 + dist))\n",
    "\n",
    "        # Generate final points and mirrored final points\n",
    "        final_coords = generate_end_points(coordinates, vertical_angles_deg, horizontal_angles_deg)\n",
    "        mirrored_final_coords = mirror_end_points(final_coords, int(n_central_piles))\n",
    "\n",
    "        # Prepare input/output points for analysis\n",
    "        initial_points = coordinates + mirror\n",
    "        final_points = np.array(final_coords + mirrored_final_coords)\n",
    "\n",
    "        # Adjust points if geometric constraints are violated\n",
    "        adjusted_final_points, horizontal_angles_deg, horizontal_angles, p = check_and_adjust_points(\n",
    "            initial_points, pile_length, minimum_distance, final_points,\n",
    "            vertical_angles_deg, horizontal_angles_deg, horizontal_angles, p\n",
    "        )\n",
    "\n",
    "        # Perform structural analysis only if no penalty\n",
    "        if p <= 0:\n",
    "            p_analysis, initial_points, adjusted_final_points, horizontal_angles, vertical_angles = analysis(\n",
    "                initial_points, adjusted_final_points, horizontal_angles, vertical_angles\n",
    "            )\n",
    "            results.append(p_analysis)\n",
    "        else:\n",
    "            results.append(p)\n",
    "\n",
    "\n",
    "    return torch.tensor(results)\n",
    "\n",
    "# TESTER\n",
    "values = [0, 0.333, 0.5, 0.666, 1] + [0.5] * 1 + [0.1] + [0.6] * 2 + [0.3] + [0.333, 0.666, 1, 0, 0.5] + [0.2] * 5\n",
    "xx = torch.tensor(values).unsqueeze(0)\n",
    "obj_fun(xx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_BSA_parameters(N_OFEs):\n",
    "    suggested_BSA_popsize = int(np.floor(np.log(N_OFEs**2)))\n",
    "    suggested_BSA_epochs = int(np.ceil(N_OFEs/np.log(N_OFEs**2)))\n",
    "    return suggested_BSA_popsize, suggested_BSA_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the optimization for EGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 20000 # epochs for training the GPR\n",
    "N_INITIAL = 250#initial number of points    \n",
    "N_INFILL = 600#number of infill points  \n",
    "N_EI_POINTS = 100000  # 1 million takes 40 seconds\n",
    "BSA_POPSIZE, BSA_EPOCH = calculate_BSA_parameters(N_EI_POINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 19  # dimension of your problem (number of design variables)\n",
    "BOUNDS_BSA = (\n",
    "    tuple((0, 1) for _ in range(DIM))  # the number of (0, 1) tuples has to be equal to DIM\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using BSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPSIZE, EPOCHS = calculate_BSA_parameters(N_INITIAL + N_INFILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bsa = bsa(obj_fun, bounds=BOUNDS_BSA,\n",
    "                    popsize=POPSIZE, epoch=EPOCHS, data=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "time_BSA = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f* = {results_bsa.y:.5f}; x* = {results_bsa.x}\")\n",
    "print(f\"Total BSA time: {time_BSA:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the cost history\n",
    "\n",
    "print(results_bsa)\n",
    "plt.figure(figsize=(8, 6))  # Optional: Set figure size\n",
    "plt.plot(np.arange(EPOCHS), results_bsa.convergence, marker='o', linestyle='-', color='b', label='Cost History')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([-0.5, 0])\n",
    "plt.xlim([0,N_INITIAL + N_INFILL])\n",
    "plt.ylabel('Cost')\n",
    "plt.title('BSA Cost History per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)  # Optional: Add grid for better visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate initial sampling plan using LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LHS_sample(N_INITIAL, DIM)\n",
    "x = torch.from_numpy(x)\n",
    "x = x.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_EGO = {\n",
    "    'LHS': 0,\n",
    "    'training': [],\n",
    "    'obj_fun': [],\n",
    "    'EI': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "name_f, name_x = 'f.mat', 'x.mat'\n",
    "path = './'\n",
    "pathname_f = path + name_f\n",
    "pathname_x = path + name_x\n",
    "check_file = os.path.isfile(pathname_f)\n",
    "if check_file:\n",
    "    mat_contents = sio.loadmat(pathname_f)\n",
    "    f = mat_contents['f']\n",
    "    f = torch.Tensor(f)\n",
    "    f = f.view(N_INITIAL)\n",
    "    \n",
    "    mat_contents = sio.loadmat(pathname_x)\n",
    "    x = mat_contents['x']\n",
    "    x = torch.Tensor(x)\n",
    "    \n",
    "else:\n",
    "    t0 = time.time()\n",
    "    f = obj_fun(x)\n",
    "    f = f.view(N_INITIAL)\n",
    "    sio.savemat(name_f, {'f': f.numpy()})\n",
    "    sio.savemat(name_x, {'x': x.numpy()})\n",
    "    t1 = time.time()\n",
    "    time_EGO['LHS'] = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_g, val_g = train_test_split(x, f, test_size=0.20)\n",
    "# Converter os dados de treino e validação para o tipo float32\n",
    "train_x = train_x.float()\n",
    "train_g = train_g.float()\n",
    "val_x = val_x.float()\n",
    "val_g = val_g.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Global Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "model, likelihood, best_loss = train_model(train_x, train_g, val_x, val_g, TRAINING_ITERATIONS)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "it = 0\n",
    "t1 = time.time()\n",
    "time_EGO['training'].append(t1 - t0)\n",
    "print(f'\\nIteration {it}. Best of the OFEs: {torch.min(f)}. GPR model: Train loss = {best_loss[0]}; Val. loss = {best_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while it < N_INFILL:\n",
    "    it += 1\n",
    "    \n",
    "    # Search for the maximum expected improvement\n",
    "    t0 = time.time()    \n",
    "    new_point = bsa(expected_improvement, bounds=BOUNDS_BSA,\n",
    "                    popsize=BSA_POPSIZE, epoch=BSA_EPOCH, data=model)\n",
    "    x_new = torch.from_numpy(new_point.x).float()  # Garantir que x_new esteja em float\n",
    "    EI = new_point.y\n",
    "    t1 = time.time()\n",
    "    time_EGO['EI'].append(t1 - t0)\n",
    "\n",
    "    # Objective function at the new point\n",
    "    t0 = time.time()\n",
    "    f_new = obj_fun(x_new.unsqueeze(0).float())  # Garante que x_new é float32 e 1D\n",
    "    f_new = f_new.view(-1)                       # Ajusta f_new para a concatenação com f\n",
    "\n",
    "    print(f'Iteration {it} of {N_INFILL}')\n",
    "    if f_new.item() < torch.min(f).item():       # Converte f_new e torch.min(f) para escalares\n",
    "        print(f'New best: f* = {float(f_new):.5f}; x* = {x_new.unsqueeze(0).float()}; at position {it}')\n",
    "    \n",
    "    # Add new values to the initial sampling\n",
    "    x = torch.cat((x, x_new.unsqueeze(0)), 0).float()\n",
    "    f = torch.cat((f, f_new), 0).float()\n",
    "    t1 = time.time()\n",
    "    time_EGO['obj_fun'].append(t1 - t0)\n",
    "    \n",
    "    # Update model\n",
    "    t0 = time.time()\n",
    "    train_x, val_x, train_g, val_g = train_test_split(x.float(), f.float(), test_size=0.20)\n",
    "    model, likelihood, best_loss = train_model(train_x, train_g, val_x, val_g, TRAINING_ITERATIONS)\n",
    "    model = model.to(torch.float32)\n",
    "    likelihood = likelihood.to(torch.float32)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    t1 = time.time()\n",
    "    time_EGO['training'].append(t1 - t0)\n",
    "    \n",
    "    print(f'\\nIteration {it}. Best of the OFEs: {torch.min(f)}. GPR model: Train loss = {best_loss[0]}; Val. loss = {best_loss[-1]}')\n",
    "    \n",
    "    # if abs(EI) < TOL_MIN_EI:\n",
    "    #     print('Optimization finished. Minimum tolerance achieved.')\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the cost history\n",
    "plt.figure(figsize=(8, 6))  # Optional: Set figure size\n",
    "plt.ylim([-0.5, 0])\n",
    "plt.xlim([0,N_INITIAL+N_INFILL])\n",
    "# plt.plot(np.arange(N_INITIAL), torch.min(f[:N_INITIAL]) * torch.ones(N_INITIAL), marker='o', linestyle='-', color='b', label='Cost History')\n",
    "\n",
    "f_best = torch.Tensor([torch.min(f[:N_INITIAL]), torch.min(f[:N_INITIAL])])\n",
    "idx_best = torch.Tensor([0, N_INITIAL])\n",
    "\n",
    "for i in range(N_INITIAL, N_INITIAL+N_INFILL):\n",
    "    if f[i] < f_best[-1]:\n",
    "        f_best = torch.cat((f_best, torch.tensor([f[i]])), dim=0)\n",
    "        idx_best = torch.cat((idx_best, torch.tensor([i])), dim=0)\n",
    "\n",
    "if idx_best[-1] < N_INITIAL + N_INFILL - 1:\n",
    "    idx_best = torch.cat((idx_best, torch.tensor([N_INITIAL + N_INFILL - 1])), dim=0)\n",
    "    f_best = torch.cat((f_best, torch.tensor([torch.min(f)])), dim=0)\n",
    "\n",
    "plt.plot(idx_best, f_best, marker='o', linestyle='-', color='k', label='Cost History')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('EGO Cost History per Epoch')\n",
    "plt.legend(['Cost'])\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)  # Optional: Add grid for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'f*: {torch.min(f):.5f}; x*: {x[torch.argmin(f), :]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_EGO['LHS'] == 0:\n",
    "    time_EGO['LHS'] = np.mean(time_EGO['obj_fun'])*N_INITIAL\n",
    "time_EGO['total'] = np.sum(time_EGO['training']) + np.sum(time_EGO['obj_fun']) + np.sum(time_EGO['LHS']) + np.sum(time_EGO['EI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_EGO['EI_total'] = np.sum(time_EGO['EI'])\n",
    "print(f\"Total EI time: {time_EGO['EI_total']:.1f} seconds ({100*time_EGO['EI_total']/time_EGO['total']:.1f}%)\")\n",
    "\n",
    "print(f\"Total Initial Sampling Plan time: {np.sum(time_EGO['LHS']):.1f} seconds ({100*time_EGO['LHS']/time_EGO['total']:.1f}%), for {N_INITIAL} sampling points\")\n",
    "\n",
    "time_EGO['obj_fun_total'] = np.sum(time_EGO['obj_fun'])\n",
    "print(f\"Total EGO OFEs time: {time_EGO['obj_fun_total']:.1f} seconds ({100*time_EGO['obj_fun_total']/time_EGO['total']:.1f}%)\")\n",
    "\n",
    "time_EGO['training_total'] = np.sum(time_EGO['training'])\n",
    "print(f\"Total training time: {time_EGO['training_total']:.1f} seconds ({100*time_EGO['training_total']/time_EGO['total']:.1f}%)\")\n",
    "\n",
    "print(f\"Total EGO time: {time_EGO['total']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"EGO OFEs vs BSA OFEs: {N_INITIAL + N_INFILL} vs {EPOCHS * POPSIZE}\")\n",
    "print(f\"EGO time vs BSA time: {time_EGO['total']} vs {time_BSA} seconds\")\n",
    "suggested_BSA_OFEs = int(np.round((N_INITIAL + N_INFILL) * time_EGO['total'] / time_BSA))\n",
    "suggested_BSA_popsize, suggested_BSA_epochs = calculate_BSA_parameters(suggested_BSA_OFEs)\n",
    "print(f\"Suggested BSA OFEs for equalling EGO time: {suggested_BSA_OFEs}\")\n",
    "print(f\"Suggested BSA popsize: {suggested_BSA_popsize} and suggested BSA epochs: {suggested_BSA_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EGO vs BSA\")\n",
    "print(f'EGO ---> f*: {torch.min(f):.5f}; x*: {x[torch.argmin(f), :]}')\n",
    "print(f\"BSA ---> f* = {results_bsa.y:.5f}; x* = {results_bsa.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results[\"x\"] = x\n",
    "results[\"f\"] = f\n",
    "results[\"n_initial\"] = N_INITIAL\n",
    "results[\"n_infill\"] = N_INFILL\n",
    "results[\"OFEs\"] = N_INITIAL + it\n",
    "results[\"time\"] = time_EGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_f, name_x = 'f.mat', 'x.mat'\n",
    "path = './'\n",
    "pathname_f = path + name_f\n",
    "pathname_x = path + name_x\n",
    "\n",
    "sio.savemat(name_f, {'f': f.numpy()})\n",
    "sio.savemat(name_x, {'x': x.numpy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BSA optimization with suggested OFEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_suggested_bsa = bsa(obj_fun, bounds=BOUNDS_BSA,\n",
    "                    popsize=suggested_BSA_popsize, epoch=suggested_BSA_epochs, data=[], tempo=time_EGO[\"total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "time_suggested_BSA = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f* = {results_suggested_bsa.y:.2f}; x* = {results_suggested_bsa.x}\")\n",
    "print(f\"Total suggested BSA time: {time_suggested_BSA:.1f} seconds\")\n",
    "print(f\"EGO time vs suggested BSA time: {time_EGO['total']} vs {time_suggested_BSA} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EGO vs BSA\")\n",
    "print(f'EGO ---> f*: {torch.min(f):.5f}; x*: {x[torch.argmin(f), :]}')\n",
    "print(f\"BSA ---> f* = {results_suggested_bsa.y:.5f}; x* = {results_suggested_bsa.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARISON BETWEEN BOTH METHODS CONSIDERING SAME OFE COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EGO_best = float(torch.min(f))\n",
    "BSA_best = float(results_bsa.y)\n",
    "a = abs(EGO_best)\n",
    "b = abs(BSA_best)\n",
    "if EGO_best < BSA_best:\n",
    "    print(f\"EGO wins! {EGO_best} vs {BSA_best}\")\n",
    "else:\n",
    "    print(f\"BSA wins! {BSA_best} vs {EGO_best}\")\n",
    "\n",
    "if a > b:\n",
    "    reduction = abs((a-b)/a)\n",
    "else:\n",
    "    reduction = abs((a-b)/b)\n",
    "\n",
    "print(f\"Reduction of {reduction*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARISON BETWEEN BOTH METHODS CONSIDERING SAME TIME COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_best = float(results_suggested_bsa.y)\n",
    "a = abs(EGO_best)\n",
    "b = abs(BSA_best)\n",
    "if EGO_best < BSA_best:\n",
    "\n",
    "    print(f\"EGO wins! {EGO_best} vs {BSA_best}\")\n",
    "else:\n",
    "    print(f\"Suggested BSA wins! {BSA_best} vs {EGO_best}\")\n",
    "\n",
    "if a > b:\n",
    "    reduction = abs((a-b)/a)\n",
    "else:\n",
    "\n",
    "    reduction = abs((a-b)/b)\n",
    "\n",
    "print(f\"Reduction of {reduction*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the cost history\n",
    "plt.figure(figsize=(8, 6))  # Optional: Set figure size\n",
    "plt.plot(np.arange(results_suggested_bsa.convergence.shape[0]), results_suggested_bsa.convergence, marker='o', linestyle='-', color='b', label='Cost History')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.ylim([-0.5, 0])\n",
    "plt.xlim([0,N_INITIAL+N_INFILL])\n",
    "plt.title('BSA Cost History per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)  # Optional: Add grid for better visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sair da port do mapdl e fechar o processo\n",
    "mapdl.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
